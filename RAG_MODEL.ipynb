{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":10387,"status":"ok","timestamp":1699622652904,"user":{"displayName":"Quân Hà Minh","userId":"14393248588275484880"},"user_tz":-420},"id":"8D0Knh13kflC"},"outputs":[],"source":["\n","# !pip install -qU \\\n","#   transformers==4.31.0 \\\n","#   sentence-transformers==2.2.2 \\\n","#   pinecone-client==2.2.2 \\\n","#   datasets==2.14.0 \\\n","#   accelerate==0.21.0 \\\n","#   einops==0.6.1 \\\n","#   langchain==0.0.240 \\\n","#   xformers==0.0.20 \\\n","#   bitsandbytes==0.41.0"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1699622672973,"user":{"displayName":"Quân Hà Minh","userId":"14393248588275484880"},"user_tz":-420},"id":"PL92PdmL2n-G"},"outputs":[{"name":"stderr","output_type":"stream","text":["e:\\Program Files\\Python310\\lib\\site-packages\\bitsandbytes\\cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n","  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"]},{"name":"stdout","output_type":"stream","text":["'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"]}],"source":["from langchain.document_loaders import PyPDFLoader\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from langchain.vectorstores import Pinecone\n","import os\n","import pandas as pd\n","from embedding_model import embed_model \n","# UnstructuredPDFLoader, OnlinePDFLoader,\n","# from dotenv import load_dotenv\n","# load_dotenv()"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":3143,"status":"ok","timestamp":1699622692201,"user":{"displayName":"Quân Hà Minh","userId":"14393248588275484880"},"user_tz":-420},"id":"kmuro5jm4qN4"},"outputs":[],"source":["loader = PyPDFLoader(\"./axir.pdf\")\n","data = loader.load()"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":300,"status":"ok","timestamp":1699622697693,"user":{"displayName":"Quân Hà Minh","userId":"14393248588275484880"},"user_tz":-420},"id":"V1nx-Rjf-6SS","outputId":"7d2daac8-6e2f-40b2-bd25-1db66207dfa0"},"outputs":[{"name":"stdout","output_type":"stream","text":["You have 67 document(s) in your data\n","There are 3721 characters in your document\n"]}],"source":["print (f'You have {len(data)} document(s) in your data')\n","print (f'There are {len(data[30].page_content)} characters in your document')"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":277,"status":"ok","timestamp":1699622704222,"user":{"displayName":"Quân Hà Minh","userId":"14393248588275484880"},"user_tz":-420},"id":"zc7IL_XZ_QQo"},"outputs":[{"name":"stdout","output_type":"stream","text":["Now you have 135 documents\n","There are 1931 characters in your document\n"]}],"source":["text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=0)\n","texts = text_splitter.split_documents(data)\n","print (f'Now you have {len(texts)} documents')\n","print (f'There are {len(texts[30].page_content)} characters in your document')"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1699622988255,"user":{"displayName":"Quân Hà Minh","userId":"14393248588275484880"},"user_tz":-420},"id":"_qXBrRIIU0H2"},"outputs":[],"source":["# page_contents = [doc.page_content for doc in texts]\n","# data1 = pd.DataFrame(page_contents, columns=['page_content'])\n","# data1.head()"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2263,"status":"ok","timestamp":1699623006987,"user":{"displayName":"Quân Hà Minh","userId":"14393248588275484880"},"user_tz":-420},"id":"rJjzrkXSV-uN","outputId":"21286ecb-1b47-49cd-9eec-bd85e2eb38b0"},"outputs":[],"source":["from datasets import load_dataset\n","data2 = load_dataset(\n","    'jamescalam/llama-2-arxiv-papers-chunked',\n","    split='train'\n",")\n","# print(data2)\n"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":764},"executionInfo":{"elapsed":317,"status":"ok","timestamp":1699623014651,"user":{"displayName":"Quân Hà Minh","userId":"14393248588275484880"},"user_tz":-420},"id":"0jCkhryJWC6H","outputId":"357ff8a0-76f8-43dd-8abd-6fdf3f23e958"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>doi</th>\n","      <th>chunk-id</th>\n","      <th>chunk</th>\n","      <th>id</th>\n","      <th>title</th>\n","      <th>summary</th>\n","      <th>source</th>\n","      <th>authors</th>\n","      <th>categories</th>\n","      <th>comment</th>\n","      <th>journal_ref</th>\n","      <th>primary_category</th>\n","      <th>published</th>\n","      <th>updated</th>\n","      <th>references</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1102.0183</td>\n","      <td>0</td>\n","      <td>High-Performance Neural Networks\\nfor Visual O...</td>\n","      <td>1102.0183</td>\n","      <td>High-Performance Neural Networks for Visual Ob...</td>\n","      <td>We present a fast, fully parameterizable GPU i...</td>\n","      <td>http://arxiv.org/pdf/1102.0183</td>\n","      <td>[Dan C. Cireşan, Ueli Meier, Jonathan Masci, L...</td>\n","      <td>[cs.AI, cs.NE]</td>\n","      <td>12 pages, 2 figures, 5 tables</td>\n","      <td>None</td>\n","      <td>cs.AI</td>\n","      <td>20110201</td>\n","      <td>20110201</td>\n","      <td>[]</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1102.0183</td>\n","      <td>1</td>\n","      <td>January 2011\\nAbstract\\nWe present a fast, ful...</td>\n","      <td>1102.0183</td>\n","      <td>High-Performance Neural Networks for Visual Ob...</td>\n","      <td>We present a fast, fully parameterizable GPU i...</td>\n","      <td>http://arxiv.org/pdf/1102.0183</td>\n","      <td>[Dan C. Cireşan, Ueli Meier, Jonathan Masci, L...</td>\n","      <td>[cs.AI, cs.NE]</td>\n","      <td>12 pages, 2 figures, 5 tables</td>\n","      <td>None</td>\n","      <td>cs.AI</td>\n","      <td>20110201</td>\n","      <td>20110201</td>\n","      <td>[]</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1102.0183</td>\n","      <td>2</td>\n","      <td>promising architectures for such tasks. The mo...</td>\n","      <td>1102.0183</td>\n","      <td>High-Performance Neural Networks for Visual Ob...</td>\n","      <td>We present a fast, fully parameterizable GPU i...</td>\n","      <td>http://arxiv.org/pdf/1102.0183</td>\n","      <td>[Dan C. Cireşan, Ueli Meier, Jonathan Masci, L...</td>\n","      <td>[cs.AI, cs.NE]</td>\n","      <td>12 pages, 2 figures, 5 tables</td>\n","      <td>None</td>\n","      <td>cs.AI</td>\n","      <td>20110201</td>\n","      <td>20110201</td>\n","      <td>[]</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1102.0183</td>\n","      <td>3</td>\n","      <td>Mutch and Lowe, 2008), whose \flters are \fxed, ...</td>\n","      <td>1102.0183</td>\n","      <td>High-Performance Neural Networks for Visual Ob...</td>\n","      <td>We present a fast, fully parameterizable GPU i...</td>\n","      <td>http://arxiv.org/pdf/1102.0183</td>\n","      <td>[Dan C. Cireşan, Ueli Meier, Jonathan Masci, L...</td>\n","      <td>[cs.AI, cs.NE]</td>\n","      <td>12 pages, 2 figures, 5 tables</td>\n","      <td>None</td>\n","      <td>cs.AI</td>\n","      <td>20110201</td>\n","      <td>20110201</td>\n","      <td>[]</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1102.0183</td>\n","      <td>4</td>\n","      <td>We evaluate various networks on the handwritte...</td>\n","      <td>1102.0183</td>\n","      <td>High-Performance Neural Networks for Visual Ob...</td>\n","      <td>We present a fast, fully parameterizable GPU i...</td>\n","      <td>http://arxiv.org/pdf/1102.0183</td>\n","      <td>[Dan C. Cireşan, Ueli Meier, Jonathan Masci, L...</td>\n","      <td>[cs.AI, cs.NE]</td>\n","      <td>12 pages, 2 figures, 5 tables</td>\n","      <td>None</td>\n","      <td>cs.AI</td>\n","      <td>20110201</td>\n","      <td>20110201</td>\n","      <td>[]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         doi chunk-id                                              chunk  \\\n","0  1102.0183        0  High-Performance Neural Networks\\nfor Visual O...   \n","1  1102.0183        1  January 2011\\nAbstract\\nWe present a fast, ful...   \n","2  1102.0183        2  promising architectures for such tasks. The mo...   \n","3  1102.0183        3  Mutch and Lowe, 2008), whose \n","lters are \n","xed, ...   \n","4  1102.0183        4  We evaluate various networks on the handwritte...   \n","\n","          id                                              title  \\\n","0  1102.0183  High-Performance Neural Networks for Visual Ob...   \n","1  1102.0183  High-Performance Neural Networks for Visual Ob...   \n","2  1102.0183  High-Performance Neural Networks for Visual Ob...   \n","3  1102.0183  High-Performance Neural Networks for Visual Ob...   \n","4  1102.0183  High-Performance Neural Networks for Visual Ob...   \n","\n","                                             summary  \\\n","0  We present a fast, fully parameterizable GPU i...   \n","1  We present a fast, fully parameterizable GPU i...   \n","2  We present a fast, fully parameterizable GPU i...   \n","3  We present a fast, fully parameterizable GPU i...   \n","4  We present a fast, fully parameterizable GPU i...   \n","\n","                           source  \\\n","0  http://arxiv.org/pdf/1102.0183   \n","1  http://arxiv.org/pdf/1102.0183   \n","2  http://arxiv.org/pdf/1102.0183   \n","3  http://arxiv.org/pdf/1102.0183   \n","4  http://arxiv.org/pdf/1102.0183   \n","\n","                                             authors      categories  \\\n","0  [Dan C. Cireşan, Ueli Meier, Jonathan Masci, L...  [cs.AI, cs.NE]   \n","1  [Dan C. Cireşan, Ueli Meier, Jonathan Masci, L...  [cs.AI, cs.NE]   \n","2  [Dan C. Cireşan, Ueli Meier, Jonathan Masci, L...  [cs.AI, cs.NE]   \n","3  [Dan C. Cireşan, Ueli Meier, Jonathan Masci, L...  [cs.AI, cs.NE]   \n","4  [Dan C. Cireşan, Ueli Meier, Jonathan Masci, L...  [cs.AI, cs.NE]   \n","\n","                         comment journal_ref primary_category published  \\\n","0  12 pages, 2 figures, 5 tables        None            cs.AI  20110201   \n","1  12 pages, 2 figures, 5 tables        None            cs.AI  20110201   \n","2  12 pages, 2 figures, 5 tables        None            cs.AI  20110201   \n","3  12 pages, 2 figures, 5 tables        None            cs.AI  20110201   \n","4  12 pages, 2 figures, 5 tables        None            cs.AI  20110201   \n","\n","    updated references  \n","0  20110201         []  \n","1  20110201         []  \n","2  20110201         []  \n","3  20110201         []  \n","4  20110201         []  "]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["data2 = data2.to_pandas()\n","data2.head()"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1699623192242,"user":{"displayName":"Quân Hà Minh","userId":"14393248588275484880"},"user_tz":-420},"id":"sxGLq_dI_abt"},"outputs":[],"source":["import os\n","import pinecone\n","\n","PINECONE_API_KEY=\"693f554c-86c4-4c59-983f-7137f9fa9faa\"\n","PINECONE_ENVIRONMENT=\"gcp-starter\"\n","# get API key from app.pinecone.io and environment from console\n","pinecone.init(\n","    api_key=os.environ.get(PINECONE_API_KEY) or PINECONE_API_KEY,\n","    environment=os.environ.get(PINECONE_ENVIRONMENT) or PINECONE_ENVIRONMENT\n",")"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":307,"status":"ok","timestamp":1699623036512,"user":{"displayName":"Quân Hà Minh","userId":"14393248588275484880"},"user_tz":-420},"id":"r4yjyqMfWeCm","outputId":"f8e365d3-6850-43b5-e3d2-21b17c65022e"},"outputs":[{"name":"stdout","output_type":"stream","text":["We have 15 doc embeddings, each with a dimensionality of 384.\n"]}],"source":["embeddings1 = embed_model.embed_documents(data2)\n","\n","print(f\"We have {len(embeddings1)} doc embeddings, each with \"\n","      f\"a dimensionality of {len(embeddings1[0])}.\")"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":16249,"status":"ok","timestamp":1699623349355,"user":{"displayName":"Quân Hà Minh","userId":"14393248588275484880"},"user_tz":-420},"id":"pB9ejcsSC-zS"},"outputs":[],"source":["import time\n","\n","index_name = 'ragmodel'\n","embeddings_dimension = len(embeddings1[0])\n","\n","if index_name not in pinecone.list_indexes():\n","    pinecone.create_index(\n","        index_name,\n","        dimension=embeddings_dimension,\n","        metric='cosine'\n","    )\n","    # wait for index to finish initialization\n","    while not pinecone.describe_index(index_name).status['ready']:\n","        time.sleep(1)"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["docsearch = Pinecone.from_documents(texts, embed_model , index_name=index_name)"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"data":{"text/plain":["[Document(page_content='Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu\\nSergeyEdunov, MyleOtt, MichaelAuli, and DavidGrangier. Understandingback-translation\\nat scale. arXiv preprint arXiv:1808.09381 , 2018.\\nEdouard Grave, Piotr Bojanowski, Prakhar Gupta, Armand Joulin, and Tomas Mikolov.\\nLearning word vectors for 157 languages. arXiv preprint arXiv:1802.06893 , 2018.\\nAlex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850 , 2013.\\nIvan Habernal, Omnia Zayed, and Iryna Gurevych. C4Corpus: Multilingual web-size corpus\\nwith free license. In Proceedings of the Tenth International Conference on Language\\nResources and Evaluation (LREC’16) , pages 914–922, 2016.\\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for\\nimage recognition. In Proceedings of the IEEE conference on computer vision and pattern\\nrecognition , 2016.\\nKaiming He, Ross Girshick, and Piotr Dollár. Rethinking ImageNet pre-training. arXiv\\npreprint arXiv:1811.08883 , 2018.\\nPengcheng He, Xiaodong Liu, Weizhu Chen, and Jianfeng Gao. A hybrid neural network\\nmodel for commonsense reasoning. arXiv preprint arXiv:1907.11983 , 2019.\\nKarl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay,\\nMustafa Suleyman, and Phil Blunsom. Teaching machines to read and comprehend. In\\nAdvances in neural information processing systems , 2015.\\nJoel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan\\nKianinejad, Md. Mostofa Ali Patwary, Yang Yang, and Yanqi Zhou. Deep learning scaling\\nis predictable, empirically. arXiv preprint arXiv:1712.00409 , 2017.\\nFelix Hill, Kyunghyun Cho, and Anna Korhonen. Learning distributed representations of\\nsentences from unlabelled data. arXiv preprint arXiv:1602.03483 , 2016.\\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network.\\narXiv preprint arXiv:1503.02531 , 2015.', metadata={'page': 59.0, 'source': './axir.pdf'}),\n"," Document(page_content='Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu\\nSergeyEdunov, MyleOtt, MichaelAuli, and DavidGrangier. Understandingback-translation\\nat scale. arXiv preprint arXiv:1808.09381 , 2018.\\nEdouard Grave, Piotr Bojanowski, Prakhar Gupta, Armand Joulin, and Tomas Mikolov.\\nLearning word vectors for 157 languages. arXiv preprint arXiv:1802.06893 , 2018.\\nAlex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850 , 2013.\\nIvan Habernal, Omnia Zayed, and Iryna Gurevych. C4Corpus: Multilingual web-size corpus\\nwith free license. In Proceedings of the Tenth International Conference on Language\\nResources and Evaluation (LREC’16) , pages 914–922, 2016.\\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for\\nimage recognition. In Proceedings of the IEEE conference on computer vision and pattern\\nrecognition , 2016.\\nKaiming He, Ross Girshick, and Piotr Dollár. Rethinking ImageNet pre-training. arXiv\\npreprint arXiv:1811.08883 , 2018.\\nPengcheng He, Xiaodong Liu, Weizhu Chen, and Jianfeng Gao. A hybrid neural network\\nmodel for commonsense reasoning. arXiv preprint arXiv:1907.11983 , 2019.\\nKarl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay,\\nMustafa Suleyman, and Phil Blunsom. Teaching machines to read and comprehend. In\\nAdvances in neural information processing systems , 2015.\\nJoel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan\\nKianinejad, Md. Mostofa Ali Patwary, Yang Yang, and Yanqi Zhou. Deep learning scaling\\nis predictable, empirically. arXiv preprint arXiv:1712.00409 , 2017.\\nFelix Hill, Kyunghyun Cho, and Anna Korhonen. Learning distributed representations of\\nsentences from unlabelled data. arXiv preprint arXiv:1602.03483 , 2016.\\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network.\\narXiv preprint arXiv:1503.02531 , 2015.', metadata={'page': 59.0, 'source': './axir.pdf'}),\n"," Document(page_content='Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu\\nSergeyEdunov, MyleOtt, MichaelAuli, and DavidGrangier. Understandingback-translation\\nat scale. arXiv preprint arXiv:1808.09381 , 2018.\\nEdouard Grave, Piotr Bojanowski, Prakhar Gupta, Armand Joulin, and Tomas Mikolov.\\nLearning word vectors for 157 languages. arXiv preprint arXiv:1802.06893 , 2018.\\nAlex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850 , 2013.\\nIvan Habernal, Omnia Zayed, and Iryna Gurevych. C4Corpus: Multilingual web-size corpus\\nwith free license. In Proceedings of the Tenth International Conference on Language\\nResources and Evaluation (LREC’16) , pages 914–922, 2016.\\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for\\nimage recognition. In Proceedings of the IEEE conference on computer vision and pattern\\nrecognition , 2016.\\nKaiming He, Ross Girshick, and Piotr Dollár. Rethinking ImageNet pre-training. arXiv\\npreprint arXiv:1811.08883 , 2018.\\nPengcheng He, Xiaodong Liu, Weizhu Chen, and Jianfeng Gao. A hybrid neural network\\nmodel for commonsense reasoning. arXiv preprint arXiv:1907.11983 , 2019.\\nKarl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay,\\nMustafa Suleyman, and Phil Blunsom. Teaching machines to read and comprehend. In\\nAdvances in neural information processing systems , 2015.\\nJoel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan\\nKianinejad, Md. Mostofa Ali Patwary, Yang Yang, and Yanqi Zhou. Deep learning scaling\\nis predictable, empirically. arXiv preprint arXiv:1712.00409 , 2017.\\nFelix Hill, Kyunghyun Cho, and Anna Korhonen. Learning distributed representations of\\nsentences from unlabelled data. arXiv preprint arXiv:1602.03483 , 2016.\\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network.\\narXiv preprint arXiv:1503.02531 , 2015.', metadata={'page': 59.0, 'source': './axir.pdf'}),\n"," Document(page_content='Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu\\nSergeyEdunov, MyleOtt, MichaelAuli, and DavidGrangier. Understandingback-translation\\nat scale. arXiv preprint arXiv:1808.09381 , 2018.\\nEdouard Grave, Piotr Bojanowski, Prakhar Gupta, Armand Joulin, and Tomas Mikolov.\\nLearning word vectors for 157 languages. arXiv preprint arXiv:1802.06893 , 2018.\\nAlex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850 , 2013.\\nIvan Habernal, Omnia Zayed, and Iryna Gurevych. C4Corpus: Multilingual web-size corpus\\nwith free license. In Proceedings of the Tenth International Conference on Language\\nResources and Evaluation (LREC’16) , pages 914–922, 2016.\\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for\\nimage recognition. In Proceedings of the IEEE conference on computer vision and pattern\\nrecognition , 2016.\\nKaiming He, Ross Girshick, and Piotr Dollár. Rethinking ImageNet pre-training. arXiv\\npreprint arXiv:1811.08883 , 2018.\\nPengcheng He, Xiaodong Liu, Weizhu Chen, and Jianfeng Gao. A hybrid neural network\\nmodel for commonsense reasoning. arXiv preprint arXiv:1907.11983 , 2019.\\nKarl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay,\\nMustafa Suleyman, and Phil Blunsom. Teaching machines to read and comprehend. In\\nAdvances in neural information processing systems , 2015.\\nJoel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan\\nKianinejad, Md. Mostofa Ali Patwary, Yang Yang, and Yanqi Zhou. Deep learning scaling\\nis predictable, empirically. arXiv preprint arXiv:1712.00409 , 2017.\\nFelix Hill, Kyunghyun Cho, and Anna Korhonen. Learning distributed representations of\\nsentences from unlabelled data. arXiv preprint arXiv:1602.03483 , 2016.\\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network.\\narXiv preprint arXiv:1503.02531 , 2015.', metadata={'page': 59.0, 'source': './axir.pdf'})]"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["query = \"deep learning\"\n","docs = docsearch.similarity_search(query)\n","docs"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":558,"status":"ok","timestamp":1699623349910,"user":{"displayName":"Quân Hà Minh","userId":"14393248588275484880"},"user_tz":-420},"id":"lDbyAKBkcXv6","outputId":"f2961fca-e9bb-4313-af70-313c35c3190a"},"outputs":[{"data":{"text/plain":["['ragmodel']"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["pinecone.list_indexes()"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1699623349911,"user":{"displayName":"Quân Hà Minh","userId":"14393248588275484880"},"user_tz":-420},"id":"nbdb7XQQW5-H","outputId":"33c33a6f-c0c8-46e0-eeab-ae62b4678f0c"},"outputs":[{"data":{"text/plain":["{'dimension': 384,\n"," 'index_fullness': 0.00604,\n"," 'namespaces': {'': {'vector_count': 604}},\n"," 'total_vector_count': 604}"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["index = pinecone.Index(index_name)\n","index.describe_index_stats()"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":34158,"status":"ok","timestamp":1699623404647,"user":{"displayName":"Quân Hà Minh","userId":"14393248588275484880"},"user_tz":-420},"id":"mnV-6yVUPomG"},"outputs":[],"source":["# batch_size = 32\n","\n","# for i in range(0, len(data2), batch_size):\n","#     i_end = min(len(data2), i+batch_size)\n","#     batch = data2.iloc[i:i_end]\n","#     ids = [f\"{x['doi']}-{x['chunk-id']}\" for i, x in batch.iterrows()]\n","#     texts = [x['chunk'] for i, x in batch.iterrows()]\n","#     embeds = embed_model.embed_documents(texts)\n","\n","#     metadata = [\n","#         {'text': x['chunk'],\n","#          'source': x['source'],\n","#          'title': x['title']} for i, x in batch.iterrows()\n","#     ]\n","\n","#     index.upsert(vectors=zip(ids, embeds, metadata))"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["\n","# from torch import cuda, bfloat16\n","# import transformers\n","\n","# model_id = 'meta-llama/Llama-2-7b-chat-hf'\n","\n","# device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n","\n","# # set quantization configuration to load large model with less GPU memory\n","# # this requires the `bitsandbytes` library\n","# bnb_config = transformers.BitsAndBytesConfig(\n","#     load_in_4bit=True,\n","#     bnb_4bit_quant_type='nf4',\n","#     bnb_4bit_use_double_quant=True,\n","#     bnb_4bit_compute_dtype=bfloat16\n","# )\n","\n","# # begin initializing HF items, need auth token for these\n","# hf_auth = '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'\n","# model_config = transformers.AutoConfig.from_pretrained(\n","#     model_id,\n","#     use_auth_token=hf_auth\n","# )\n","\n","# model = transformers.AutoModelForCausalLM.from_pretrained(\n","#     model_id,\n","#     cache_dir=\"E:/Visual studio project/Disord_chatbot/next13-discord-clone/chatbot/model\",\n","#     trust_remote_code=True,\n","#     config=model_config,\n","#     quantization_config=None,\n","#     device_map='auto',\n","#     use_auth_token=hf_auth\n","# )\n","# model.eval()\n","# print(f\"Model loaded on {device}\")"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["# tokenizer = transformers.AutoTokenizer.from_pretrained(\n","#     model_id,\n","#     cache_dir=\"E:/Visual studio project/Disord_chatbot/next13-discord-clone/chatbot/model\",\n","#     use_auth_token=hf_auth\n","# )"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["# generate_text = transformers.pipeline(\n","#     model=model, tokenizer=tokenizer,\n","#     return_full_text=True,  # langchain expects the full text\n","#     task='text-generation',\n","#     cache_dir=\"E:/Visual studio project/Disord_chatbot/next13-discord-clone/chatbot/model\",\n","#     # we pass model parameters here too\n","#     temperature=0.0,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n","#     max_new_tokens=512,  # mex number of tokens to generate in the output\n","#     repetition_penalty=1.1  # without this output begins repeating\n","# )"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["# from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer  \n","# import torch\n","# model_path = \"vinai/PhoGPT-7B5-Instruct\"  \n","  \n","# config = AutoConfig.from_pretrained(model_path, trust_remote_code=True,use_auth_token=hf_auth)  \n","# config.init_device = \"cuda\"\n","# # config.attn_config['attn_impl'] = 'triton' # Enable if \"triton\" installed!\n","  \n","# model = AutoModelForCausalLM.from_pretrained(  \n","#     model_path, config=config, torch_dtype=torch.bfloat16, trust_remote_code=True,use_auth_token=hf_auth ,cache_dir=\"E:/Visual studio project/Disord_chatbot/next13-discord-clone/chatbot/model\" \n","# )\n","# # If your GPU does not support bfloat16:\n","# # model = AutoModelForCausalLM.from_pretrained(model_path, config=config, torch_dtype=torch.float16, trust_remote_code=True)\n","# model.eval()  \n","  \n","# tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True,cache_dir=\"E:/Visual studio project/Disord_chatbot/next13-discord-clone/chatbot/model\")  \n"," "]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":[" \n","# PROMPT = \"### Câu hỏi:\\n{instruction}\\n\\n### Trả lời:\"  \n","  \n","# input_prompt = PROMPT.format_map(  \n","#     {\"instruction\": \"Làm thế nào để cải thiện kỹ năng quản lý thời gian?\"}  \n","# )  \n","  \n","# input_ids = tokenizer(input_prompt, return_tensors=\"pt\")  \n","  \n","# outputs = model.generate(  \n","#     inputs=input_ids[\"input_ids\"].to(\"cuda\"),  \n","#     attention_mask=input_ids[\"attention_mask\"].to(\"cuda\"),  \n","#     do_sample=True,  \n","#     temperature=1.0,  \n","#     top_k=50,  \n","#     top_p=0.9,  \n","#     max_new_tokens=1024,  \n","#     eos_token_id=tokenizer.eos_token_id,  \n","#     pad_token_id=tokenizer.pad_token_id  \n","# )  \n","  \n","# response = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]  \n","# response = response.split(\"### Trả lời:\")[1] "]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["# # Use a pipeline as a high-level helper\n","# from transformers import pipeline\n","\n","# pipe = pipeline(\"text-generation\", model=\"VietnamAIHub/Vietnamese_llama2_7B_8K_SFT_General_domain\" ,trust_remote_code=True)\n","# # Load model directly\n","# from transformers import AutoTokenizer, AutoModelForCausalLM\n","\n","# tokenizer = AutoTokenizer.from_pretrained(\"VietnamAIHub/Vietnamese_llama2_7B_8K_SFT_General_domain\")\n","# model = AutoModelForCausalLM.from_pretrained(\"VietnamAIHub/Vietnamese_llama2_7B_8K_SFT_General_domain\")"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["# from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n","\n","# # Tên mô hình bạn muốn tải\n","# model_name = \"VietnamAIHub/Vietnamese_llama2_7B_8K_SFT_General_domain\"\n","\n","# # Tạo một Pipeline\n","# pipe = pipeline(\"text-generation\", model=model_name, trust_remote_code=True)\n","\n","# # Đường dẫn đến thư mục bạn muốn lưu trữ mô hình và tokenizer\n","# output_dir = \"E:/Visual studio project/Disord_chatbot/next13-discord-clone/chatbot/model\"\n","\n","# # Sao chép mô hình từ thư mục mặc định đến thư mục bạn đã chọn\n","# pipe.model.save_pretrained(output_dir)\n","\n","# # Sao chép tokenizer từ thư mục mặc định đến thư mục bạn đã chọn\n","# tokenizer = AutoTokenizer.from_pretrained(model_name)\n","# tokenizer.save_pretrained(output_dir)\n","\n","# # Bây giờ, bạn có thể sử dụng tokenizer và model từ thư mục bạn đã chọn\n"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fe46e6edd35543879b0e9725b167c491","version_major":2,"version_minor":0},"text/plain":["Downloading config.json:   0%|          | 0.00/1.19k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a10015c284274073a1664ad68a85bb8a","version_major":2,"version_minor":0},"text/plain":["Downloading configuration_mpt.py:   0%|          | 0.00/11.0k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["A new version of the following files was downloaded from https://huggingface.co/vinai/PhoGPT-7B5:\n","- configuration_mpt.py\n",". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9a449fb06c164991afec68ab90db719b","version_major":2,"version_minor":0},"text/plain":["Downloading modeling_mpt.py:   0%|          | 0.00/20.1k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["A new version of the following files was downloaded from https://huggingface.co/vinai/PhoGPT-7B5:\n","- modeling_mpt.py\n",". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3d2ccdaf3a4346b1aca20caa4819e73a","version_major":2,"version_minor":0},"text/plain":["Downloading (…)model.bin.index.json:   0%|          | 0.00/31.5k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b85643d6e6f243a6830b6fa79a45dcd2","version_major":2,"version_minor":0},"text/plain":["Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fb46e5b351ec44619f8339fdbd045f2f","version_major":2,"version_minor":0},"text/plain":["Downloading (…)l-00001-of-00002.bin:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"eefd701612734b0b9974d9418f41f235","version_major":2,"version_minor":0},"text/plain":["Downloading (…)l-00002-of-00002.bin:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"ename":"AssertionError","evalue":"Torch not compiled with CUDA enabled","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)","\u001b[1;32me:\\Visual studio project\\Disord_chatbot\\next13-discord-clone\\chatbot\\RAG_MODEL.ipynb Cell 24\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Visual%20studio%20project/Disord_chatbot/next13-discord-clone/chatbot/RAG_MODEL.ipynb#X32sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m config\u001b[39m.\u001b[39minit_device \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Visual%20studio%20project/Disord_chatbot/next13-discord-clone/chatbot/RAG_MODEL.ipynb#X32sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# config.attn_config['attn_impl'] = 'triton' # Enable if \"triton\" installed!\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/Visual%20studio%20project/Disord_chatbot/next13-discord-clone/chatbot/RAG_MODEL.ipynb#X32sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m model \u001b[39m=\u001b[39m AutoModelForCausalLM\u001b[39m.\u001b[39;49mfrom_pretrained(  \n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Visual%20studio%20project/Disord_chatbot/next13-discord-clone/chatbot/RAG_MODEL.ipynb#X32sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     model_path, config\u001b[39m=\u001b[39;49mconfig, torch_dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mbfloat16, trust_remote_code\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,cache_dir\u001b[39m=\u001b[39;49mcache_dir,use_auth_token\u001b[39m=\u001b[39;49mhf_auth \n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Visual%20studio%20project/Disord_chatbot/next13-discord-clone/chatbot/RAG_MODEL.ipynb#X32sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Visual%20studio%20project/Disord_chatbot/next13-discord-clone/chatbot/RAG_MODEL.ipynb#X32sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# If your GPU does not support bfloat16:\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Visual%20studio%20project/Disord_chatbot/next13-discord-clone/chatbot/RAG_MODEL.ipynb#X32sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m# model = AutoModelForCausalLM.from_pretrained(model_path, config=config, torch_dtype=torch.float16, trust_remote_code=True)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Visual%20studio%20project/Disord_chatbot/next13-discord-clone/chatbot/RAG_MODEL.ipynb#X32sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m model\u001b[39m.\u001b[39meval()  \n","File \u001b[1;32me:\\Program Files\\Python310\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:488\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    486\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    487\u001b[0m         \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mregister(config\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m, model_class, exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m--> 488\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    489\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39mmodel_args, config\u001b[39m=\u001b[39mconfig, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mhub_kwargs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    490\u001b[0m     )\n\u001b[0;32m    491\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m    492\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n","File \u001b[1;32me:\\Program Files\\Python310\\lib\\site-packages\\transformers\\modeling_utils.py:2700\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   2697\u001b[0m     init_contexts\u001b[39m.\u001b[39mappend(init_empty_weights())\n\u001b[0;32m   2699\u001b[0m \u001b[39mwith\u001b[39;00m ContextManagers(init_contexts):\n\u001b[1;32m-> 2700\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m(config, \u001b[39m*\u001b[39mmodel_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[0;32m   2702\u001b[0m \u001b[39m# Check first if we are `from_pt`\u001b[39;00m\n\u001b[0;32m   2703\u001b[0m \u001b[39mif\u001b[39;00m use_keep_in_fp32_modules:\n","File \u001b[1;32m~/.cache\\huggingface\\modules\\transformers_modules\\vinai\\PhoGPT-7B5\\7f5dfc17c4eab0e9acfa17564dafcffc4316bd74\\modeling_mpt.py:235\u001b[0m, in \u001b[0;36mMPTForCausalLM.__init__\u001b[1;34m(self, config)\u001b[0m\n\u001b[0;32m    233\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mMPTForCausalLM only supports tied word embeddings\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    234\u001b[0m log\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mInstantiating an MPTForCausalLM model from \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m__file__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m--> 235\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer: MPTModel \u001b[39m=\u001b[39m MPTModel(config)\n\u001b[0;32m    236\u001b[0m \u001b[39mfor\u001b[39;00m child \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m    237\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(child, torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModuleList):\n","File \u001b[1;32m~/.cache\\huggingface\\modules\\transformers_modules\\vinai\\PhoGPT-7B5\\7f5dfc17c4eab0e9acfa17564dafcffc4316bd74\\modeling_mpt.py:59\u001b[0m, in \u001b[0;36mMPTModel.__init__\u001b[1;34m(self, config)\u001b[0m\n\u001b[0;32m     57\u001b[0m norm_class \u001b[39m=\u001b[39m NORM_CLASS_REGISTRY[config\u001b[39m.\u001b[39mnorm_type\u001b[39m.\u001b[39mlower()]\n\u001b[0;32m     58\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding_fraction \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39membedding_fraction\n\u001b[1;32m---> 59\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwte \u001b[39m=\u001b[39m SharedEmbedding(config\u001b[39m.\u001b[39;49mvocab_size, config\u001b[39m.\u001b[39;49md_model, device\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49minit_device)\n\u001b[0;32m     60\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlearned_pos_emb:\n\u001b[0;32m     61\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwpe \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mEmbedding(config\u001b[39m.\u001b[39mmax_seq_len, config\u001b[39m.\u001b[39md_model, device\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39minit_device)\n","File \u001b[1;32me:\\Program Files\\Python310\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:142\u001b[0m, in \u001b[0;36mEmbedding.__init__\u001b[1;34m(self, num_embeddings, embedding_dim, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse, _weight, _freeze, device, dtype)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscale_grad_by_freq \u001b[39m=\u001b[39m scale_grad_by_freq\n\u001b[0;32m    141\u001b[0m \u001b[39mif\u001b[39;00m _weight \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 142\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight \u001b[39m=\u001b[39m Parameter(torch\u001b[39m.\u001b[39mempty((num_embeddings, embedding_dim), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfactory_kwargs),\n\u001b[0;32m    143\u001b[0m                             requires_grad\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m _freeze)\n\u001b[0;32m    144\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreset_parameters()\n\u001b[0;32m    145\u001b[0m \u001b[39melse\u001b[39;00m:\n","File \u001b[1;32me:\\Program Files\\Python310\\lib\\site-packages\\torch\\cuda\\__init__.py:239\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    235\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    236\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    237\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmultiprocessing, you must use the \u001b[39m\u001b[39m'\u001b[39m\u001b[39mspawn\u001b[39m\u001b[39m'\u001b[39m\u001b[39m start method\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    238\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(torch\u001b[39m.\u001b[39m_C, \u001b[39m'\u001b[39m\u001b[39m_cuda_getDeviceCount\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m--> 239\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTorch not compiled with CUDA enabled\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    240\u001b[0m \u001b[39mif\u001b[39;00m _cudart \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    241\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\n\u001b[0;32m    242\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[39m\"\u001b[39m)\n","\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"]}],"source":["from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer  \n","import torch\n","hf_auth = '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'\n","\n","model_path = \"vinai/PhoGPT-7B5\"  \n","cache_dir = \"E:/Visual studio project/Disord_chatbot/next13-discord-clone/chatbot/model\"\n","config = AutoConfig.from_pretrained(model_path, trust_remote_code=True,use_auth_token=hf_auth,cache_dir=cache_dir)  \n","config.init_device = \"cuda\"\n","# config.attn_config['attn_impl'] = 'triton' # Enable if \"triton\" installed!\n","  \n","model = AutoModelForCausalLM.from_pretrained(  \n","    model_path, config=config, torch_dtype=torch.bfloat16, trust_remote_code=True,cache_dir=cache_dir,use_auth_token=hf_auth \n",")\n","# If your GPU does not support bfloat16:\n","# model = AutoModelForCausalLM.from_pretrained(model_path, config=config, torch_dtype=torch.float16, trust_remote_code=True)\n","model.eval()  \n","  \n","tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True,cache_dir=cache_dir,use_auth_token=hf_auth  )  \n","  \n","PROMPT = \"### Câu hỏi:\\n{instruction}\\n\\n### Trả lời:\"  \n","  \n","input_prompt = PROMPT.format_map(  \n","    {\"instruction\": \"Làm thế nào để cải thiện kỹ năng quản lý thời gian?\"}  \n",")  \n","  \n","input_ids = tokenizer(input_prompt, return_tensors=\"pt\")  \n","  \n","outputs = model.generate(  \n","    inputs=input_ids[\"input_ids\"].to(\"cuda\"),  \n","    attention_mask=input_ids[\"attention_mask\"].to(\"cuda\"),  \n","    do_sample=True,  \n","    temperature=1.0,  \n","    top_k=50,  \n","    top_p=0.9,  \n","    max_new_tokens=1024,  \n","    eos_token_id=tokenizer.eos_token_id,  \n","    pad_token_id=tokenizer.pad_token_id  \n",")  \n","  \n","response = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]  \n","response = response.split(\"### Trả lời:\")[1]"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["PyTorch version: 2.1.0+cu121\n","CUDA available: False\n"]},{"name":"stderr","output_type":"stream","text":["e:\\Program Files\\Python310\\lib\\site-packages\\torch\\cuda\\__init__.py:138: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 9010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ..\\c10\\cuda\\CUDAFunctions.cpp:108.)\n","  return torch._C._cuda_getDeviceCount() > 0\n"]}],"source":["import torch\n","\n","print(f\"PyTorch version: {torch.__version__}\")\n","print(f\"CUDA available: {torch.cuda.is_available()}\")\n"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyPJIRzQDa6HcBdA1oUR4Y3A","gpuType":"T4","mount_file_id":"12OTkT2E8l4rI0oEcS9mf26VjcZe5NfGP","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":0}
